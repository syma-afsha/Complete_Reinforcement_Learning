{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPEZLsBKXcmvlLQcnyRSoek",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syma-afsha/Complete_Reinforcement_Learning/blob/main/MDP_PolicyIteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qQCphtm_tG_N",
        "outputId": "68996e29-dd10-4703-dac9-350bb2e8fd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy and State Values:\n",
            "State: 1, Optimal Policy: walk, Optimal Value: -4.317333333384255\n",
            "State: 2, Optimal Policy: walk, Optimal Value: -4.146666666692127\n",
            "State: 3, Optimal Policy: walk, Optimal Value: -3.933333333346064\n",
            "State: 4, Optimal Policy: walk, Optimal Value: -3.666666666673032\n",
            "State: 5, Optimal Policy: tram, Optimal Value: -3.3333333333365163\n",
            "State: 6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State: 7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State: 8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State: 9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State: 10, Optimal Policy: None, Optimal Value: 0.00\n"
          ]
        }
      ],
      "source": [
        "\n",
        "class MDP(object):\n",
        "    # Initialize the MDP with N number of states (blocks)\n",
        "    def __init__(self, N):\n",
        "        self.N = N  # Number of blocks (states)\n",
        "\n",
        "    # Define the start state (always starts at state 1)\n",
        "    def StartState(self):\n",
        "        return 1\n",
        "\n",
        "    # Check if the given state is the terminal state (goal state)\n",
        "    def isEnd(self, state):\n",
        "        return state == self.N\n",
        "\n",
        "    # Define possible actions for a given state\n",
        "    def Actions(self, state):\n",
        "        result = []\n",
        "        # If walking to the next state is valid, add \"walk\" action\n",
        "        if state + 1 <= self.N:\n",
        "            result.append(\"walk\")\n",
        "        # If using the tram to double the state is valid, add \"tram\" action\n",
        "        if state * 2 <= self.N:\n",
        "            result.append(\"tram\")\n",
        "        return result\n",
        "\n",
        "    # Define transition probabilities, rewards, and next states for each action\n",
        "    def TransitionProb(self, state, action):\n",
        "        result = []\n",
        "        if action == \"walk\":\n",
        "            # \"walk\" always transitions to state+1 with probability 1 and a reward of -1\n",
        "            result.append((state + 1, 1, -1))\n",
        "        if action == \"tram\":  # action == \"tram\"\n",
        "            # \"tram\" has a 50% chance to double the state and a 50% chance to fail\n",
        "            result.append((state * 2, 0.5, -2))  # Successful tram\n",
        "            result.append((state, 0.5, -2))  # Failed tram (stay in the same state)\n",
        "        return result\n",
        "\n",
        "    # Define the discount factor (Î³), which reduces the value of future rewards\n",
        "    def discount(self):\n",
        "        return 0.8\n",
        "\n",
        "    # Return a list of all possible states (1 to N)\n",
        "    def states(self):\n",
        "        return range(1, self.N + 1)\n",
        "\n",
        "# Perform Policy Iteration to compute optimal state values and policy\n",
        "def PolicyIteration(mdp, epsilon=1e-10):\n",
        "    # Initialize policy and state values\n",
        "    policy = {state: \"walk\" for state in mdp.states() if not mdp.isEnd(state)}\n",
        "    policy[mdp.N] = None  # Terminal state has no policy\n",
        "    V = {state: 0 for state in mdp.states()}\n",
        "\n",
        "    # Function to compute Q-value for a given state and action\n",
        "    def Q(state, action):\n",
        "        return sum(prob * (reward + mdp.discount() * V[newState])\n",
        "                   for newState, prob, reward in mdp.TransitionProb(state, action))\n",
        "\n",
        "    while True:\n",
        "        # Policy Evaluation: Iteratively compute v_pi(s)\n",
        "        while True:\n",
        "            delta = 0\n",
        "            newV = V.copy()  # Create a copy to store new values\n",
        "            for state in mdp.states():\n",
        "                if mdp.isEnd(state):\n",
        "                    newV[state] = 0\n",
        "                else:\n",
        "                    # Compute the value of the state under the current policy\n",
        "                    action = policy[state]\n",
        "                    newV[state] = Q(state, action)\n",
        "\n",
        "                # Update delta to track the maximum change\n",
        "                delta = max(delta, abs(V[state] - newV[state]))\n",
        "\n",
        "            V = newV  # Update all state values after processing all states\n",
        "            if delta < epsilon:\n",
        "                break\n",
        "\n",
        "        # Policy Improvement: Update the policy to be greedy w.r.t. V(s)\n",
        "        policy_stable = True\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                best_action = None\n",
        "            else:\n",
        "              # Find the best action based on the current value function\n",
        "               best_action = max((Q(state, action), action) for action in mdp.Actions(state))[1]\n",
        "\n",
        "\n",
        "            if policy[state] != best_action:\n",
        "                policy_stable = False\n",
        "                policy[state] = best_action\n",
        "\n",
        "        # If the policy is stable, we've found the optimal policy\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    # Print the final results\n",
        "    print(\"Optimal Policy and State Values:\")\n",
        "    for state in mdp.states():\n",
        "        if mdp.isEnd(state):\n",
        "            print(f\"State: {state}, Optimal Policy: None, Optimal Value: {V[state]:.2f}\")\n",
        "        else:\n",
        "            print(f\"State: {state}, Optimal Policy: {policy[state]}, Optimal Value: {V[state]}\")\n",
        "\n",
        "# Create an MDP with 10 states (blocks)\n",
        "mdp = MDP(N=10)\n",
        "PolicyIteration(mdp)\n"
      ]
    }
  ]
}