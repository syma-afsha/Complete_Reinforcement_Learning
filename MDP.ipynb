{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkx4q4LpwdXWnPH4Vxa8UO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syma-afsha/Complete_Reinforcement_Learning/blob/main/MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Markov Decision Process (MDP) with\n",
        "ùëÅ states (blocks) and two possible actions for moving between states:\n",
        "\n",
        "\n",
        "\"walk\": Moves the state forward by 1 (deterministic, costs\n",
        "‚àí\n",
        "1\n",
        "‚àí1).\n",
        "\n",
        "\"tram\": Attempts to double the state (50% chance of success, 50% chance of failure, costs\n",
        "‚àí\n",
        "2\n",
        "‚àí2).\n",
        "\n",
        "The goal is to find the optimal policy and state values to minimize the total penalty (maximize rewards) starting from state 1 and ending at the terminal state\n",
        "ùëÅ."
      ],
      "metadata": {
        "id": "Xdt5SM2FE4O1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6uHI0nstsog",
        "outputId": "8d9dde4d-6153-4da9-8451-e8d5c76c97de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 1\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(2, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(2, 0.5, -2), (1, 0.5, -2)]\n",
            "State: 2\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(3, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(4, 0.5, -2), (2, 0.5, -2)]\n",
            "State: 3\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(4, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(6, 0.5, -2), (3, 0.5, -2)]\n",
            "State: 4\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(5, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(8, 0.5, -2), (4, 0.5, -2)]\n",
            "State: 5\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(6, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(10, 0.5, -2), (5, 0.5, -2)]\n",
            "State: 6\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(7, 1, -1)]\n",
            "State: 7\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(8, 1, -1)]\n",
            "State: 8\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(9, 1, -1)]\n",
            "State: 9\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(10, 1, -1)]\n",
            "State: 10\n",
            "  Actions: []\n"
          ]
        }
      ],
      "source": [
        "class MDP(object):\n",
        "    # Initialize the MDP with N number of states (blocks)\n",
        "    def __init__(self, N):\n",
        "        self.N = N  # Number of blocks (states)\n",
        "\n",
        "    # Define the start state (always starts at state 1)\n",
        "    def StartState(self):\n",
        "        return 1\n",
        "\n",
        "    # Check if the given state is the terminal state (goal state)\n",
        "    def isEnd(self, state):\n",
        "        return state == self.N\n",
        "\n",
        "    # Define possible actions for a given state\n",
        "    def Actions(self, state):\n",
        "        result = []\n",
        "        # If walking to the next state is valid, add \"walk\" action\n",
        "        if state + 1 <= self.N:\n",
        "            result.append(\"walk\")\n",
        "        # If using the tram to double the state is valid, add \"tram\" action\n",
        "        if state * 2 <= self.N:\n",
        "            result.append(\"tram\")\n",
        "        return result\n",
        "\n",
        "    # Define transition probabilities, rewards, and next states for each action\n",
        "    def TransitionProb(self, state, action):\n",
        "        result = []\n",
        "        if action == \"walk\":\n",
        "            # \"walk\" always transitions to state+1 with probability 1 and a reward of -1\n",
        "            result.append((state + 1, 1, -1))\n",
        "        if action == \"tram\":  # action == \"tram\"\n",
        "            # \"tram\" has a 50% chance to double the state and a 50% chance to fail\n",
        "            result.append((state * 2, 0.5, -2))  # Successful tram\n",
        "            result.append((state, 0.5, -2))  # Failed tram (stay in the same state)\n",
        "        return result\n",
        "\n",
        "    # Define the discount factor (Œ≥), which reduces the value of future rewards\n",
        "    def discount(self):\n",
        "        return 1.0\n",
        "\n",
        "    # Return a list of all possible states (1 to N)\n",
        "    def states(self):\n",
        "        return range(1, self.N + 1)\n",
        "\n",
        "# Perform Value Iteration to compute optimal state values and policy\n",
        "def ValueIteration(mdp):\n",
        "    # Initialize state values (V) to 0 for all states\n",
        "    V = {}\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0\n",
        "\n",
        "    # Function to compute Q-value for a given state and action\n",
        "    def Q(state, action):\n",
        "        # Q(s, a) = Œ£ (probability * (reward + Œ≥ * V(new state)))\n",
        "        return sum(prob * (reward + mdp.discount() * V[newState])\n",
        "                   for newState, prob, reward in mdp.TransitionProb(state, action))\n",
        "\n",
        "    while True:\n",
        "        # Initialize a new dictionary for updated state values\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                # Terminal state has a value of 0\n",
        "                newV[state] = 0\n",
        "            else:\n",
        "                # Update state value as the maximum Q-value across all actions\n",
        "                newV[state] = max(Q(state, action) for action in mdp.Actions(state))\n",
        "\n",
        "        # Check for convergence (difference between old and new values is small)\n",
        "        if max(abs(V[state] - newV[state]) for state in mdp.states()) < 1e-10:\n",
        "            break\n",
        "\n",
        "        # Update state values\n",
        "        V = newV\n",
        "\n",
        "        # Compute the optimal policy based on the updated values\n",
        "        policy = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                # Terminal state has no action\n",
        "                policy[state] = None\n",
        "            else:\n",
        "                # Optimal action is the one with the highest Q-value\n",
        "                policy[state] = max((Q(state, action), action) for action in mdp.Actions(state))[1]\n",
        "\n",
        "        # Print the current state, optimal action, and value for each state\n",
        "        for states in mdp.states():\n",
        "            print(f\"State:{states}, Optimal Policy: {policy[states]}, Optimal Value: {V[states]}\")\n",
        "        print(\"Done\")\n",
        "# Create an MDP with 10 states (blocks)\n",
        "mdp = MDP(N=10)\n",
        "for state in mdp.states():\n",
        "    print(f\"State: {state}\")\n",
        "    actions = mdp.Actions(state)\n",
        "    print(f\"  Actions: {actions}\")\n",
        "    for action in actions:\n",
        "        transitions = mdp.TransitionProb(state, action)\n",
        "        print(f\"Action: {action}, Transition Probabilities: {transitions}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Value Iteration to find the optimal policy and state values\n",
        "ValueIteration(mdp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidn_EEOl98h",
        "outputId": "eee59898-f5c9-49f7-c6e7-cddcd93daa9b"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:1, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:5, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:6, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:5, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.0\n",
            "State:6, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.5\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -5.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -5.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.5\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.75\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -6.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.5\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.75\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.5\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.75\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.5\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.75\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.96875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.75\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.96875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.984375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.96875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.984375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9921875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.96875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.984375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9921875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.99609375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.96875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.984375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9921875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.99609375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.998046875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.984375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9921875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.99609375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.998046875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9990234375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9921875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.99609375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.998046875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9990234375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.99951171875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.99609375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.998046875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9990234375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.99951171875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999755859375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.998046875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9990234375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.99951171875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999755859375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9998779296875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9990234375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.99951171875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999755859375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9998779296875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.99993896484375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.99951171875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999755859375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9998779296875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.99993896484375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999969482421875\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999755859375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9998779296875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.99993896484375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999969482421875\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999847412109375\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9998779296875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.99993896484375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999969482421875\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9999847412109375\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999923706054688\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.99993896484375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999969482421875\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9999847412109375\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999992370605469\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999961853027344\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999969482421875\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9999847412109375\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999992370605469\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999996185302734\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999998092651367\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9999847412109375\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999992370605469\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999996185302734\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999998092651367\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999990463256836\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999992370605469\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999996185302734\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999998092651367\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999046325684\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999523162842\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999996185302734\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999998092651367\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999046325684\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999523162842\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999761581421\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999998092651367\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999046325684\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999523162842\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999761581421\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999998807907104\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999046325684\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999523162842\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999761581421\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.9999998807907104\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999403953552\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999523162842\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999761581421\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.9999998807907104\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999940395355\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999701976776\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999761581421\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.9999998807907104\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999940395355\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999970197678\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999985098839\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.9999998807907104\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999940395355\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999970197678\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999985098839\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999925494194\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999940395355\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999970197678\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999985098839\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999992549419\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999962747097\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999970197678\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999985098839\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999992549419\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.99999999627471\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999998137355\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999985098839\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999992549419\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.99999999627471\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999998137355\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999990686774\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999992549419\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.99999999627471\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999998137355\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999068677\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999995343387\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.99999999627471\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999998137355\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999068677\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999534339\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999997671694\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999998137355\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999999068677\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999534339\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999767169\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999998835847\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999999068677\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999999534339\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999767169\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999883585\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.9999999999417923\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999999534339\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999999767169\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999883585\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999941792\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999999970896\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999999767169\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999999883585\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999941792\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999970896\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999999985448\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -7.999999999883585\n",
            "State:2, Optimal Policy: walk, Optimal Value: -6.999999999941792\n",
            "State:3, Optimal Policy: walk, Optimal Value: -5.999999999970896\n",
            "State:4, Optimal Policy: walk, Optimal Value: -4.999999999985448\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.999999999992724\n",
            "State:6, Optimal Policy: walk, Optimal Value: -4.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -3.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -2.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}