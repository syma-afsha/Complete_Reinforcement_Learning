{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNybZMO8esP9Q3ApPucoq9R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/syma-afsha/Complete_Reinforcement_Learning/blob/main/MDP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a Markov Decision Process (MDP) with\n",
        "ùëÅ states (blocks) and two possible actions for moving between states:\n",
        "\n",
        "\n",
        "\"walk\": Moves the state forward by 1 (deterministic, costs\n",
        "‚àí\n",
        "1\n",
        "‚àí1).\n",
        "\n",
        "\"tram\": Attempts to double the state (50% chance of success, 50% chance of failure, costs\n",
        "‚àí\n",
        "2\n",
        "‚àí2).\n",
        "\n",
        "The goal is to find the optimal policy and state values to minimize the total penalty (maximize rewards) starting from state 1 and ending at the terminal state\n",
        "ùëÅ."
      ],
      "metadata": {
        "id": "Xdt5SM2FE4O1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6uHI0nstsog",
        "outputId": "111ad027-9156-4d46-b2a7-6444bd5bbd77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 1\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(2, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(2, 0.5, -2), (1, 0.5, -2)]\n",
            "State: 2\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(3, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(4, 0.5, -2), (2, 0.5, -2)]\n",
            "State: 3\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(4, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(6, 0.5, -2), (3, 0.5, -2)]\n",
            "State: 4\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(5, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(8, 0.5, -2), (4, 0.5, -2)]\n",
            "State: 5\n",
            "  Actions: ['walk', 'tram']\n",
            "Action: walk, Transition Probabilities: [(6, 1, -1)]\n",
            "Action: tram, Transition Probabilities: [(10, 0.5, -2), (5, 0.5, -2)]\n",
            "State: 6\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(7, 1, -1)]\n",
            "State: 7\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(8, 1, -1)]\n",
            "State: 8\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(9, 1, -1)]\n",
            "State: 9\n",
            "  Actions: ['walk']\n",
            "Action: walk, Transition Probabilities: [(10, 1, -1)]\n",
            "State: 10\n",
            "  Actions: []\n"
          ]
        }
      ],
      "source": [
        "class MDP(object):\n",
        "    # Initialize the MDP with N number of states (blocks)\n",
        "    def __init__(self, N):\n",
        "        self.N = N  # Number of blocks (states)\n",
        "\n",
        "    # Define the start state (always starts at state 1)\n",
        "    def StartState(self):\n",
        "        return 1\n",
        "\n",
        "    # Check if the given state is the terminal state (goal state)\n",
        "    def isEnd(self, state):\n",
        "        return state == self.N\n",
        "\n",
        "    # Define possible actions for a given state\n",
        "    def Actions(self, state):\n",
        "        result = []\n",
        "        # If walking to the next state is valid, add \"walk\" action\n",
        "        if state + 1 <= self.N:\n",
        "            result.append(\"walk\")\n",
        "        # If using the tram to double the state is valid, add \"tram\" action\n",
        "        if state * 2 <= self.N:\n",
        "            result.append(\"tram\")\n",
        "        return result\n",
        "\n",
        "    # Define transition probabilities, rewards, and next states for each action\n",
        "    def TransitionProb(self, state, action):\n",
        "        result = []\n",
        "        if action == \"walk\":\n",
        "            # \"walk\" always transitions to state+1 with probability 1 and a reward of -1\n",
        "            result.append((state + 1, 1, -1))\n",
        "        if action == \"tram\":  # action == \"tram\"\n",
        "            # \"tram\" has a 50% chance to double the state and a 50% chance to fail\n",
        "            result.append((state * 2, 0.5, -2))  # Successful tram\n",
        "            result.append((state, 0.5, -2))  # Failed tram (stay in the same state)\n",
        "        return result\n",
        "\n",
        "    # Define the discount factor (Œ≥), which reduces the value of future rewards\n",
        "    def discount(self):\n",
        "        return 0.8\n",
        "\n",
        "    # Return a list of all possible states (1 to N)\n",
        "    def states(self):\n",
        "        return range(1, self.N + 1)\n",
        "\n",
        "# Perform Value Iteration to compute optimal state values and policy\n",
        "def ValueIteration(mdp):\n",
        "    # Initialize state values (V) to 0 for all states\n",
        "    V = {}\n",
        "    for state in mdp.states():\n",
        "        V[state] = 0\n",
        "\n",
        "    # Function to compute Q-value for a given state and action\n",
        "    def Q(state, action):\n",
        "        # Q(s, a) = Œ£ (probability * (reward + Œ≥ * V(new state)))\n",
        "        return sum(prob * (reward + mdp.discount() * V[newState])\n",
        "                   for newState, prob, reward in mdp.TransitionProb(state, action))\n",
        "\n",
        "    while True:\n",
        "        # Initialize a new dictionary for updated state values\n",
        "        newV = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                # Terminal state has a value of 0\n",
        "                newV[state] = 0\n",
        "            else:\n",
        "                # Update state value as the maximum Q-value across all actions\n",
        "                newV[state] = max(Q(state, action) for action in mdp.Actions(state))\n",
        "\n",
        "        # Check for convergence (difference between old and new values is small)\n",
        "        if max(abs(V[state] - newV[state]) for state in mdp.states()) < 1e-10:\n",
        "            break\n",
        "\n",
        "        # Update state values\n",
        "        V = newV\n",
        "\n",
        "        # Compute the optimal policy based on the updated values\n",
        "        policy = {}\n",
        "        for state in mdp.states():\n",
        "            if mdp.isEnd(state):\n",
        "                # Terminal state has no action\n",
        "                policy[state] = None\n",
        "            else:\n",
        "                # Optimal action is the one with the highest Q-value\n",
        "                policy[state] = max((Q(state, action), action) for action in mdp.Actions(state))[1]\n",
        "\n",
        "        # Print the current state, optimal action, and value for each state\n",
        "        for states in mdp.states():\n",
        "            print(f\"State:{states}, Optimal Policy: {policy[states]}, Optimal Value: {V[states]}\")\n",
        "        print(\"Done\")\n",
        "# Create an MDP with 10 states (blocks)\n",
        "mdp = MDP(N=10)\n",
        "for state in mdp.states():\n",
        "    print(f\"State: {state}\")\n",
        "    actions = mdp.Actions(state)\n",
        "    print(f\"  Actions: {actions}\")\n",
        "    for action in actions:\n",
        "        transitions = mdp.TransitionProb(state, action)\n",
        "        print(f\"Action: {action}, Transition Probabilities: {transitions}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Value Iteration to find the optimal policy and state values\n",
        "ValueIteration(mdp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eidn_EEOl98h",
        "outputId": "0d71858e-9ec9-4759-f63e-0a04be249929"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State:1, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:2, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:3, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:4, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:5, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:6, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:7, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:2, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:3, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:4, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:5, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:6, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:7, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:2, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:3, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:4, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:5, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:2, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:3, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:4, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:5, Optimal Policy: tram, Optimal Value: -2.9520000000000004\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -3.3616000000000006\n",
            "State:2, Optimal Policy: walk, Optimal Value: -3.3616000000000006\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.3616000000000006\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.3616000000000006\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.1808000000000005\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -3.6892800000000006\n",
            "State:2, Optimal Policy: walk, Optimal Value: -3.6892800000000006\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.6892800000000006\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.5446400000000007\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.2723200000000006\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -3.9514240000000007\n",
            "State:2, Optimal Policy: walk, Optimal Value: -3.9514240000000007\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.835712000000001\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6178560000000006\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3089280000000003\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.161139200000001\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.068569600000001\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.8942848000000008\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6471424000000003\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3235712\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.25485568\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.115427840000001\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9177139200000006\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.65885696\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.32942848\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.292342272000001\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.134171136000001\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9270855680000003\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.663542784\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3317713920000003\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.307336908800001\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.1416684544\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9308342272\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6654171136\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3327085568\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.31333476352\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.14466738176\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.93233369088\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6661668454400003\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.33308342272\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.315733905408\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.145866952704001\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9329334763520003\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666466738176\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333233369088\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.3166935621632\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146346781081601\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9331733905408\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6665866952704\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3332933476352\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.31707742486528\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.1465387124326405\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.93326935621632\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.66663467810816\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.33331733905408\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317230969946113\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146615484973056\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333077424865284\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666538712432644\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333326935621632\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317292387978445\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146646193989223\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333230969946116\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666615484973057\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333330774248653\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317316955191378\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.14665847759569\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333292387978447\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666646193989223\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333332309699461\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317326782076552\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146663391038276\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.933331695519138\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666658477595693\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333329238797846\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317330712830621\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.14666535641531\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333326782076554\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666666339103828\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333333169551914\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317332285132249\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666142566124\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333330712830623\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666665356415313\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333332678207657\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.3173329140528995\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.14666645702645\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333332285132254\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666666614256613\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333071283064\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333165621161\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.14666658281058\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333332914052903\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666457026453\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333228513227\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333266248465\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666633124232\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333165621163\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666582810583\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333333329140529\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.3173333064993855\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666653249693\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.933333326624847\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666633124234\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333316562115\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.3173333225997546\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666661299878\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333306499387\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666653249695\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333326624848\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.3173333290399025\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666664519952\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333322599757\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666666666129988\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333333333064994\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333331615961\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666665807981\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333329039903\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666666666451995\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333332259976\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333332646385\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666666323192\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333331615963\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666665807983\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.3333333332903994\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333333058554\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666666529278\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.9333333332646387\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.6666666666323198\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.33333333331616\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n",
            "State:1, Optimal Policy: walk, Optimal Value: -4.317333333223422\n",
            "State:2, Optimal Policy: walk, Optimal Value: -4.146666666611711\n",
            "State:3, Optimal Policy: walk, Optimal Value: -3.933333333305856\n",
            "State:4, Optimal Policy: walk, Optimal Value: -3.666666666652928\n",
            "State:5, Optimal Policy: tram, Optimal Value: -3.333333333326464\n",
            "State:6, Optimal Policy: walk, Optimal Value: -2.9520000000000004\n",
            "State:7, Optimal Policy: walk, Optimal Value: -2.4400000000000004\n",
            "State:8, Optimal Policy: walk, Optimal Value: -1.8\n",
            "State:9, Optimal Policy: walk, Optimal Value: -1.0\n",
            "State:10, Optimal Policy: None, Optimal Value: 0\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}